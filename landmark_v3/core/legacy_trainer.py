import os
import time
import torch
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# --- ğŸ”¥ å¼•ç”¨ä¿®æ­£ ---
from utils import legacy_utils as MyUtils
from core import legacy_loss as LossFunction

class Trainer:
    def __init__(self, config, coarse_net, fine_LSTM, dataloaders, criterion_coarse, criterion_fine, optimizer):
        self.config = config
        self.coarse_net = coarse_net
        self.fine_LSTM = fine_LSTM
        self.dataloaders = dataloaders
        self.criterion_coarse = criterion_coarse
        self.criterion_fine = criterion_fine
        self.optimizer = optimizer

    def run(self):
        """
        å¯¹åº”æ—§ä»£ç ä¸­çš„ train_model å‡½æ•°
        """
        # ä¸ºäº†ä¸æ”¹åŠ¨ä¸‹é¢çš„é€»è¾‘ä»£ç ï¼Œæˆ‘ä»¬æŠŠ self.xxx èµ‹å€¼ç»™å±€éƒ¨å˜é‡
        # è¿™æ ·ä¸‹é¢çš„ä»£ç å‡ ä¹å¯ä»¥åŸæ ·ç²˜è´´
        config = self.config
        coarse_net = self.coarse_net
        fine_LSTM = self.fine_LSTM
        dataloaders = self.dataloaders
        criterion_coarse = self.criterion_coarse
        criterion_fine = self.criterion_fine
        optimizer = self.optimizer
        
        since = time.time()
        test_epoch = 1         # epochä¸º5çš„å€æ•°çš„æ—¶å€™ï¼ŒéªŒè¯æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ
        best_mre = float('inf')     # æœ€ä½³MRE
        start_epoch = 0         # é»˜è®¤ä» epoch0 å¼€å§‹

        # --- 1. å‡†å¤‡ä¿å­˜è·¯å¾„ ---
        save_dir = os.path.join('runs', config.saveName)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # --- 2. åˆå§‹åŒ– Logger (å…³é”®ä¿®æ”¹) ---
        # æ—¥å¿—æ–‡ä»¶å°†ä¿å­˜åœ¨ runs/ä½ çš„å®éªŒå/train.log
        log_path = os.path.join(save_dir, 'train.log')
        # è°ƒç”¨æˆ‘ä»¬åœ¨ MyUtils é‡Œå†™çš„å‡½æ•°
        logger = MyUtils.get_logger(log_path) 
        logger.info(f"ğŸš€ Start Training: {config.saveName}")
        logger.info(f"ğŸ“ Logs and weights will be saved to: {save_dir}")
        logger.info("")

        # åˆå§‹åŒ– SummaryWriter
        writer = SummaryWriter(log_dir=os.path.join('runs', config.saveName))
        # tensorboardè®°å½•é…ç½®å‚æ•°
        config_str = " | Parameter | Value |\n|---|---|\n"
        for key, value in vars(config).items():
            config_str += f"| {key} | {str(value)} |\n"
        writer.add_text('Experiment_Config', config_str, 0)

        # é…ç½®å‚æ•°ä¹Ÿè®°å½•åˆ° logger
        logger.info("------ Experiment Configuration ------")
        for key, value in vars(config).items():
            logger.info(f"{key}: {value}")
        logger.info("--------------------------------------")
        logger.info("")

        # --- å‡†å¤‡å…¨å±€åæ ‡ç½‘æ ¼ ---
        gl, gh, gw = config.image_scale
        global_coordinate = torch.ones(gl, gh, gw, 3).float()
        for i in range(gl): global_coordinate[i, :, :, 0] *= i
        for i in range(gh): global_coordinate[:, i, :, 1] *= i
        for i in range(gw): global_coordinate[:, :, i, 2] *= i
        global_coordinate = global_coordinate.cuda(config.use_gpu) * torch.tensor([1 / (gl - 1), 1 / (gh - 1), 1 / (gw - 1)]).cuda(config.use_gpu)

        # --- æ–­ç‚¹åŠ è½½é€»è¾‘ (Resume) ---
        if config.resume:
            checkpoint_path = config.resume
            if os.path.isdir(checkpoint_path):   # å¦‚æœä¼ å…¥çš„æ˜¯æ–‡ä»¶å¤¹ (ä¾‹å¦‚: runs/test4)ï¼Œè‡ªåŠ¨æ‹¼æ¥æ–‡ä»¶å
                checkpoint_path = os.path.join(checkpoint_path, 'latest_checkpoint.pth')
            
            if os.path.isfile(checkpoint_path):  # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                logger.info(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path)    # åŠ è½½ checkpoint
                # æ¢å¤çŠ¶æ€
                start_epoch = checkpoint['epoch'] + 1
                best_mre = checkpoint['best_mre']
                coarse_net.load_state_dict(checkpoint['coarse_state_dict'])
                fine_LSTM.load_state_dict(checkpoint['fine_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                logger.info(f"âœ… Resumed training from Epoch {start_epoch}. Previous Best MRE: {best_mre:.4f}")
            else:
                logger.warning(f"âš ï¸ No checkpoint found at '{checkpoint_path}'. Starting from scratch.")

        # --- è®­ç»ƒå¾ªç¯ ---
        for epoch in range(start_epoch, config.epochs):     # ä»start_epochå¼€å§‹è®­ç»ƒ
            print() # æ¯è½®å¼€å§‹åœ¨æ§åˆ¶å°æ‰“å°ä¸€ä¸ªç©ºè¡Œéš”å¼€ï¼Œlogé‡Œé¢ä¸ç”¨ç®¡
            train_coarse_Off = []
            train_fine_Off = []
            test_coarse_Off = []
            test_fine_Off = []
            
            for phase in ['train', 'val']:
                datas = dataloaders[phase]  # ç›´æ¥è·å– DataLoader
                pbar = tqdm(total=len(datas), desc=f'{phase} Epoch {epoch}') # æ‰‹åŠ¨ç®¡ç† tqdmï¼Œä¿æŒåŸå§‹é£æ ¼

                if phase == 'train':
                    coarse_net.train(True) # å¼€å¯è®­ç»ƒæ¨¡å¼
                    fine_LSTM.train(True)
                else:
                    if epoch % test_epoch != 0: continue
                    coarse_net.train(False)    # å¼€å¯æµ‹è¯•æ¨¡å¼
                    fine_LSTM.train(False)

                lent = len(datas)
                running_loss = 0

                # éå†æ•°æ®
                for data in datas:
                    inputs = data['DICOM'].cuda(config.use_gpu) # (B, C, D, H, W)
                    inputs_origin_list = data['DICOM_origin']
                    
                    # âš ï¸ æ³¨æ„: DataLoader å¦‚æœ batch_size > 1ï¼ŒDICOM_origin å¯èƒ½ä¼šè¢« stack æˆ Tensor
                    # æ—§ä»£ç é€»è¾‘ä¼¼ä¹æœŸæœ›å®ƒæ˜¯ä¸€ä¸ª list of tensorsã€‚
                    # å¦‚æœ legacy_dataset.py é‡Œæ²¡æœ‰æ”¹ collate_fnï¼Œè¿™é‡Œ inputs_origin_list å¯èƒ½æ˜¯ä¸€ä¸ª 5D Tensor (B, C, D, H, W)
                    # ä¸‹é¢è¿™è¡Œæ—§ä»£ç æš—ç¤º inputs_origin_list æ˜¯ä¸€ä¸ª list æˆ–è€… Tensor
                    # inputs_origin = [item.squeeze(0) for item in inputs_origin_list] 
                    # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬å‡è®¾ legacy_dataset çš„è¾“å‡ºæ˜¯ç¬¦åˆè¿™é‡Œé¢„æœŸçš„
                    # å¦‚æœ inputs_origin_list æ˜¯ tensor (B, 1, 512, 512, 512)
                    if isinstance(inputs_origin_list, torch.Tensor):        # å¦‚æœæ˜¯tensor
                        inputs_origin = [inputs_origin_list[b] for b in range(inputs_origin_list.shape[0])]
                    else:       # å¦‚æœæ˜¯list
                        inputs_origin = [item.squeeze(0) for item in inputs_origin_list]

                    labels = data['landmarks'].cuda(config.use_gpu) 
               
                    size = data['size'][0]
                    px_z, px_y, px_x = size[0].item(), size[1].item(), size[2].item()
                    
                    # æ„é€ åƒç´ å°ºå¯¸å¼ é‡
                    size_tensor_pixel = torch.tensor([px_x, px_y, px_z]).float().cuda(config.use_gpu).unsqueeze(0)   # Label é¡ºåºæ˜¯ [X, Y, Z],ç¼©æ”¾å› å­ä¹Ÿå¿…é¡»å¯¹åº” [Width, Height, Depth]
                    size_tensor_inv = 1.0 / size_tensor_pixel.float()

                    # ç‰©ç†å°ºå¯¸å¼ é‡ (ç”¨äºè®¡ç®— MRE æ¯«ç±³è¯¯å·®)  é€»è¾‘: åƒç´ æ•° * Spacing = æ¯«ç±³æ•°
                    sp_z, sp_y, sp_x = config.spacing # ä»é…ç½®è¯»å–
                    physical_scale = torch.tensor([
                        px_x * sp_x, # Width (mm)
                        px_y * sp_y, # Height (mm)
                        px_z * sp_z  # Depth (mm)
                    ]).float().cuda(config.use_gpu).unsqueeze(0)
                    
                    optimizer.zero_grad()

                    # æ˜¾å­˜æ§åˆ¶ï¼šéªŒè¯é˜¶æ®µä¸æ„å»ºè®¡ç®—å›¾ é˜²æ­¢éªŒè¯é›†åƒæ‰æ˜¾å­˜
                    with torch.set_grad_enabled(phase == 'train'):
                        # å‰å‘ä¼ æ’­ (Forward)
                        coarse_heatmap, coarse_feature = coarse_net(inputs)

                        # ç¬¬ä¸€é“å…³å¡ï¼šæ£€æŸ¥ç½‘ç»œè¾“å‡ºæ˜¯å¦æ­£å¸¸ 
                        has_nan = any(torch.isnan(h).any() for h in coarse_heatmap)     # æ£€æŸ¥ list ä¸­ä»»ä½•ä¸€ä¸ª tensor æ˜¯å¦æœ‰ NaN
                        if has_nan:     # å¦‚æœ CoarseNet è¾“å‡ºé‡Œå°±æœ‰ NaNï¼Œè¯´æ˜ç½‘ç»œå†…éƒ¨ç‚¸äº† æ­¤æ—¶å¿…é¡»è·³è¿‡ï¼Œä¸èƒ½æŠŠ NaN ä¼ ç»™ MyUtilsï¼Œå¦åˆ™ä¼šçˆ†å†…å­˜
                            logger.warning(f"âš ï¸ [Warning] NaN detected in CoarseNet output at Epoch {epoch}. Skipping this batch.")
                            optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
                            continue # ğŸ”¥ ç›´æ¥è·³è¿‡ï¼ä¸è·‘ FineNetï¼Œä¸åå‘ä¼ æ’­
                        
                        # è·å–ç²—å®šä½åæ ‡
                        coarse_landmarks = MyUtils.get_coordinates_from_coarse_heatmaps(coarse_heatmap, global_coordinate)      # è¿™é‡Œæœ‰æ²¡æœ‰.unsqueeze(0)
                        # å¼ºåˆ¶é™åˆ¶åæ ‡åœ¨ 0-1 ï¼Œé˜²æ­¢é¢„æµ‹è·‘å‡ºè¾¹ç•Œ
                        coarse_landmarks = torch.clamp(coarse_landmarks, 0.0, 1.0)
                        
                        # Fine Stage
                        fine_landmarks_all = fine_LSTM(coarse_landmarks, labels, inputs_origin, coarse_feature, phase, size_tensor_inv)

                        # è®¡ç®— Loss (Original Logic)
                        mask_loss = (labels[:, :, 0] >= 0).float().unsqueeze(2)
                        # å–æœ€åä¸€æ¬¡è¿­ä»£çš„ç»“æœæ¥è®¡ç®—æŸå¤±
                        fine_pred_last = fine_landmarks_all[-1].unsqueeze(0)

                        # Fine Loss: ä½¿ç”¨ SmoothL1Loss
                        smooth_l1_loss = torch.nn.SmoothL1Loss(reduction='none')    # æ³¨æ„: reduction='none' é…åˆ mask æ‰‹åŠ¨æ±‚å’Œ
                        loss_fine_raw = smooth_l1_loss(fine_pred_last, labels)
                        loss_fine = (loss_fine_raw * mask_loss).sum() / (mask_loss.sum() + 1e-6)
                        # Coarse Loss: ä¿æŒåŸæ · (Heatmap Loss)
                        loss_coarse = criterion_coarse(coarse_heatmap, global_coordinate, labels, phase)
                        # Total Loss: åŠ æƒæ±‚å’Œ (FineNet æƒé‡ç¿»å€)
                        loss = 2.0 * loss_fine + 1.0 * loss_coarse      # ç›®çš„: å‘Šè¯‰æ¨¡å‹ "Coarse å·®ä¸å¤šå°±è¡Œï¼Œä½† Fine å¿…é¡»å‡†"

                        # åå‘ä¼ æ’­
                        if phase == 'train' and config.stage == 'train':
                            # ç¬¬äºŒé“å…³å¡ï¼šæ£€æŸ¥ Loss æ˜¯å¦æ­£å¸¸
                            if torch.isnan(loss):
                                logger.warning(f"âš ï¸ [Warning] Loss is NaN at Epoch {epoch}. Skipping gradient update.")
                                optimizer.zero_grad()
                                continue # ğŸ”¥ è·³è¿‡æ›´æ–°
                            loss.backward()
                            # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´ä¸‹ä¸€æ¬¡é¢„æµ‹é£å‡ºå¤©é™…)
                            torch.nn.utils.clip_grad_norm_(coarse_net.parameters(), max_norm=5.0)
                            torch.nn.utils.clip_grad_norm_(fine_LSTM.parameters(), max_norm=5.0)
                            optimizer.step()

                    # -------------------------------------------------------------------
                    # 4. æŒ‡æ ‡ç»Ÿè®¡
                    # -------------------------------------------------------------------
                    if epoch % test_epoch == 0:
                        fine_landmarks_final = fine_landmarks_all[-1].unsqueeze(0)    # åªå–æœ€åä¸€æ¬¡è¿­ä»£çš„ç»“æœç”¨äºè¯„ä¼°
                        coarse_off = MyUtils.get_coarse_errors(coarse_landmarks, labels, physical_scale) # è®¡ç®—è¯¯å·®ï¼Œç›´æ¥ä¼ å…¥ç‰©ç†å°ºå¯¸ï¼ˆmmï¼‰
                        fine_off = MyUtils.get_fine_errors(fine_landmarks_final, labels, physical_scale)

                        # å¤„ç†ç¼ºå¤±å€¼ï¼šå¡«å…… NaN
                        mask_valid = (labels[:, :, 0] >= 0) # [1, N]
                        coarse_off[~mask_valid] = float('nan')
                        fine_off[~mask_valid] = float('nan')
                        
                        if phase == "train":
                            train_fine_Off.append(fine_off.detach().cpu())
                            train_coarse_Off.append(coarse_off.detach().cpu())
                        else:
                            test_fine_Off.append(fine_off.detach().cpu())
                            test_coarse_Off.append(coarse_off.detach().cpu())
                    
                    running_loss += loss.item()
                    pbar.update(1)

                # End of Epoch
                epoch_loss = running_loss / lent
                pbar.close()
                
                if epoch % 1 == 0:
                    logger.info('{} epoch: {} Loss: {:.4f}'.format(phase, epoch, epoch_loss))
                
                if phase == 'train':
                    writer.add_scalar('Loss/Train', epoch_loss, epoch)
                elif phase == 'val':
                    writer.add_scalar('Loss/Val', epoch_loss, epoch)

            # -------------------------------------------------------------------
            # 5. TensorBoard è®°å½•ä¸ç»“æœä¿å­˜
            # -------------------------------------------------------------------
            if epoch % test_epoch == 0:
                current_test_mre = float('inf')
                # --- å†…éƒ¨å‡½æ•°ï¼šè®¡ç®—å¹¶è®°å½•æŒ‡æ ‡ (é¿å…ä»£ç é‡å¤) ---
                def process_stats(tensor_list, prefix):
                    if len(tensor_list) == 0: return float('inf'), float('inf'), None
                    # 1. æ‹¼æ¥ [Total_N, 7] (åŒ…å« NaN)
                    all_tensor = torch.cat(tensor_list, dim=0)
                    # 2. MyUtils è®¡ç®—ç»†èŠ‚ (SDR, æ¯åˆ—å‡å€¼)
                    SDR, _, _ = MyUtils.analysis_result(config.landmarkNum, all_tensor)
                    # 3. è®¡ç®—å…¨å±€æŒ‡æ ‡ (Micro-Average, å¿½ç•¥ NaN)
                    global_mre = torch.nanmean(all_tensor).item()
                    # è®¡ç®—å…¨å±€ SD (å…¼å®¹æ—§ç‰ˆ PyTorch çš„æ‰‹åŠ¨ nanstd)
                    global_sd = torch.std(all_tensor[~torch.isnan(all_tensor)]).item()
                    # 4. è®°å½• TensorBoard
                    writer.add_scalar(f'{prefix}_MRE', global_mre, epoch)
                    writer.add_scalar(f'{prefix}_SD', global_sd, epoch)
                    # è®°å½• SDR (å–æ‰€æœ‰å…³é”®ç‚¹å¹³å‡)
                    sdr_mean = torch.mean(SDR, dim=0) * 100
                    writer.add_scalar(f'{prefix}_SDR/2.0mm', sdr_mean[1], epoch) # å‡è®¾é˜ˆå€¼ç´¢å¼•1å¯¹åº”2.0mm
                    writer.add_scalar(f'{prefix}_SDR/4.0mm', sdr_mean[3], epoch)
                    writer.add_scalar(f'{prefix}_SDR/6.0mm', sdr_mean[5], epoch)
                    writer.add_scalar(f'{prefix}_SDR/8.0mm', sdr_mean[7], epoch)
                    return global_mre, global_sd, sdr_mean
                # --- å†…éƒ¨å‡½æ•°ï¼šè¾“å‡ºæŒ‡æ ‡ç»“æœ (é¿å…ä»£ç é‡å¤) ---
                def print_detailed_results(title, c_mre, c_sd, f_mre, f_sd, sdr_vec):
                    logger.info(f"   [{title} Results]")
                    logger.info(f"   Fine   -> MRE: {f_mre:.4f} mm | SD: {f_sd:.4f} mm")
                    logger.info(f"   Coarse -> MRE: {c_mre:.4f} mm | SD: {c_sd:.4f} mm")
                    logger.info(f"   SDR (Thresholds):")
                    logger.info(f"     2.0mm:{sdr_vec[1]:.2f}%")
                    logger.info(f"     4.0mm:{sdr_vec[3]:.2f}%")
                    logger.info(f"     6.0mm:{sdr_vec[5]:.2f}%")
                    logger.info(f"     8.0mm:{sdr_vec[7]:.2f}%")
                    logger.info(f"   Full SDR Vector: {sdr_vec.tolist()}")
                    logger.info("")
                    
                # å¤„ç† Train
                c_mre_train, c_sd_train, _ = process_stats(train_coarse_Off, 'Train/Coarse')
                f_mre_train, f_sd_train, sdr_train = process_stats(train_fine_Off, 'Train/Fine')

                # å¤„ç† Val/Test ç»“æœ
                c_mre_test, c_sd_test, _ = process_stats(test_coarse_Off, 'Test/Coarse')
                f_mre_test, f_sd_test, sdr_test = process_stats(test_fine_Off, 'Test/Fine')
                
                # è®°å½•å¯¹æ¯”æ›²çº¿
                writer.add_scalars('Comparison/MRE_Train', {'Coarse': c_mre_train, 'Fine': f_mre_train}, epoch)
                writer.add_scalars('Comparison/MRE_Test', {'Coarse': c_mre_test, 'Fine': f_mre_test}, epoch)

                current_test_mre = f_mre_test # ä»¥ Fine MRE ä¸ºå‡†

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if current_test_mre < best_mre:
                    logger.info(f"ğŸ”¥ New Best! MRE: {best_mre:.4f} -> {current_test_mre:.4f} (Epoch {epoch})")
                    best_mre = current_test_mre
                    torch.save(coarse_net.state_dict(), os.path.join(save_dir, 'best_coarse.pth'))
                    torch.save(fine_LSTM.state_dict(), os.path.join(save_dir, 'best_fine_LSTM.pth'))
                
                if (epoch + 1) % 1 == 0:       # æ¯10ä¸ªepochæ‰“å°è¾“å‡ºä¸€ä¸‹è¯„ä»·æŒ‡æ ‡
                    logger.info("")
                    print_detailed_results("TRAIN", c_mre_train, c_sd_train, f_mre_train, f_sd_train, sdr_train)
                    print_detailed_results("TEST ", c_mre_test, c_sd_test, f_mre_test, f_sd_test, sdr_test)

                # --- ğŸ”¥ ä¿å­˜æ–­ç‚¹ (åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€å’ŒEpoch) ---
                checkpoint_state = {
                    'epoch': epoch,
                    'coarse_state_dict': coarse_net.state_dict(),
                    'fine_state_dict': fine_LSTM.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(), # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
                    'best_mre': best_mre
                }
                torch.save(checkpoint_state, os.path.join(save_dir, 'latest_checkpoint.pth'), _use_new_zipfile_serialization=False)
                logger.info(f"ğŸ’¾ Checkpoint saved: epoch {epoch}")

            logger.info("")     # æ‰“å°ç©ºè¡Œï¼Œä¸ºäº†ç»ˆç«¯æ˜¾ç¤ºç¾è§‚ï¼Œæ—¥å¿—é‡Œé¢ä¼šæœ‰ä¸ªç©ºè¡Œ
            torch.cuda.empty_cache()
        
        time_elapsed = time.time() - since
        logger.info('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        writer.close()
    
    def test(self):
        """
        å¯¹åº”æ—§ä»£ç ä¸­çš„ test_model å‡½æ•°
        """
        # æœ¬åœ°å˜é‡æ˜ å°„
        config = self.config
        coarse_net = self.coarse_net
        fine_LSTM = self.fine_LSTM
        dataloader = self.dataloaders['val'] # å‡è®¾ test ä½¿ç”¨ val_loader

        # ==========================================
        # ğŸ‘‡ ä¸‹é¢å¼€å§‹å®Œå…¨å¤åˆ» test_model é€»è¾‘
        # ==========================================
        since = time.time()

        coarse_net.eval()
        fine_LSTM.eval()
        
        # 1. å‡†å¤‡å…¨å±€åæ ‡ç½‘æ ¼ (ä¸ TrainNet ä¿æŒä¸€è‡´)
        gl, gh, gw = config.image_scale
        global_coordinate = torch.ones(gl, gh, gw, 3).float()
        for i in range(gl): global_coordinate[i, :, :, 0] *= i
        for i in range(gh): global_coordinate[:, i, :, 1] *= i
        for i in range(gw): global_coordinate[:, :, i, 2] *= i
        global_coordinate = global_coordinate.cuda(config.use_gpu) * torch.tensor([1 / (gl - 1), 1 / (gh - 1), 1 / (gw - 1)]).cuda(config.use_gpu)

        # å®¹å™¨
        coarse_Off = []
        fine_Off = []

        # --- 1. å‡†å¤‡ä¿å­˜è·¯å¾„ ---
        # æ³¨æ„ï¼šè¿™é‡Œç”¨ testNameï¼Œå¦‚æœæ²¡æœ‰å®šä¹‰åœ¨ config é‡Œï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ä¸€ä¸‹
        # ä¸ºäº†å…¼å®¹ï¼Œå¦‚æœ config æ²¡æœ‰ testNameï¼Œå°±ç”¨ saveName + '_test'
        test_name = getattr(config, 'testName', config.saveName + '_test')
        save_dir = os.path.join('runs', test_name)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # --- 2. åˆå§‹åŒ– Logger ---   æ—¥å¿—æ–‡ä»¶å°†ä¿å­˜åœ¨ runs/ä½ çš„å®éªŒå/test.log
        log_path = os.path.join(save_dir, 'test.log')
        # è°ƒç”¨æˆ‘ä»¬åœ¨ MyUtils é‡Œå†™çš„å‡½æ•°
        logger = MyUtils.get_logger(log_path) 
        logger.info(f"ğŸš€ Start Testing: {test_name}")
        logger.info(f"ğŸ“ Logs will be saved to: {save_dir}")
        landmark_names = ["AICV", "ANS", "BEP", "PNS", "TEE", "TEP", "TUV"]
        header = f"{'Sample ID':<25} | {'Coarse MRE':<12} | {'Fine MRE':<12} | {'Scale (mm)':<25}"
        for name in landmark_names: header += f" || {name[:4]+'_C':<10} | {name[:4]+'_F':<10}"     # ä¿æŒåˆ—å®½ç´§å‡‘ï¼Œ_Cä»£è¡¨Coarse, _Fä»£è¡¨Fine
        logger.info(header)
        logger.info("-" * len(header))

        with torch.no_grad():
            for i, data in enumerate(dataloader):
                inputs = data['DICOM'].cuda(config.use_gpu)
                labels = data['landmarks'].cuda(config.use_gpu)
                inputs_origin_list = data['DICOM_origin']
                
                # å…¼å®¹ tensor/list è¾“å…¥
                if isinstance(inputs_origin_list, torch.Tensor):
                    inputs_origin = [inputs_origin_list[b] for b in range(inputs_origin_list.shape[0])]
                else:
                    inputs_origin = [item.squeeze(0) for item in inputs_origin_list]
                
                # --- æ„å»ºå°ºå¯¸å¼ é‡ ---
                # 1. è·å– Batch ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å°ºå¯¸ (å‡è®¾ Batch=1)
                size_data = data['size'][0] 
                px_z, px_y, px_x = size_data[0].item(), size_data[1].item(), size_data[2].item()
                
                # 2. åƒç´ å°ºå¯¸å¼ é‡ (ç”¨äº FineNet è¾“å…¥/å½’ä¸€åŒ–) - é¡ºåº [W, H, D] å¯¹åº” [x, y, z]
                size_tensor_pixel = torch.tensor([px_x, px_y, px_z]).float().cuda(config.use_gpu).unsqueeze(0)
                size_tensor_inv = 1.0 / size_tensor_pixel

                # 3. ç‰©ç†å°ºå¯¸å¼ é‡ (ç”¨äºè®¡ç®— MRE æ¯«ç±³è¯¯å·®)
                sp_z, sp_y, sp_x = config.spacing
                physical_scale = torch.tensor([
                    px_x * sp_x, # Width (mm)
                    px_y * sp_y, # Height (mm)
                    px_z * sp_z  # Depth (mm)
                ]).float().cuda(config.use_gpu).unsqueeze(0)

                # --- æ¨ç† ---
                # 1. Coarse Stage
                coarse_heatmap, coarse_feature = coarse_net(inputs)
                
                # 2. Get Coarse Coordinates
                # coarse_landmarks = MyUtils.get_coordinates_from_coarse_heatmaps(coarse_heatmap, global_coordinate).unsqueeze(0)
                coarse_landmarks = MyUtils.get_coordinates_from_coarse_heatmaps(coarse_heatmap, global_coordinate)
                coarse_landmarks = torch.clamp(coarse_landmarks, 0.0, 1.0)
                
                # 3. Fine Stage
                fine_landmarks = fine_LSTM(coarse_landmarks, labels, inputs_origin, coarse_feature, 'test', size_tensor_inv)    # è¿™é‡Œå¾—åˆ°çš„ä¸‰æ¬¡è¿­ä»£çš„ä¸‰ä¸ªç»“æœ
                fine_landmarks = fine_landmarks[-1].unsqueeze(0)    # åªå–æœ€åä¸€æ¬¡è¿­ä»£çš„ç»“æœ

                # --- è®¡ç®—è¯¯å·® (mm) ---     è¿™é‡Œçš„errå½¢çŠ¶æ˜¯ (B, N)ï¼ŒåŒ…å«äº†æ— æ•ˆç‚¹çš„å·¨å¤§è¯¯å·®
                c_err = MyUtils.get_coarse_errors(coarse_landmarks, labels, physical_scale)
                f_err = MyUtils.get_fine_errors(fine_landmarks, labels, physical_scale)

                # ä½¿ç”¨ Mask è¿‡æ»¤æ— æ•ˆç‚¹ï¼Œä¿ç•™æœ‰æ•ˆç‚¹ï¼Œæ— æ•ˆä½ç½®è®¾ç½®ä¸º nan
                mask = (labels[:, :, 0] >= 0) # å½¢çŠ¶ (B, N)
                c_err[~mask] = float('nan')
                f_err[~mask] = float('nan')
                
                # è®°å½•æ•°æ®
                coarse_Off.append(c_err.cpu())
                fine_Off.append(f_err.cpu())
                
                # æ‰“å°å•ä¸ªæ ·æœ¬ä¿¡æ¯ (å¯é€‰)
                sample_name = data['imageName'][0] if 'imageName' in data else "Unknown"
                c_mre_sample = torch.nanmean(c_err).item()      # torch.nanmeanå¯ä»¥ç›´æ¥å¤„ç† tensorä¸­çš„ nan
                f_mre_sample = torch.nanmean(f_err).item()
                scale_str = f"[{physical_scale[0,0]:.1f}, {physical_scale[0,1]:.1f}, {physical_scale[0,2]:.1f}]"
                row_str = f"{sample_name[:25]:<25} | {c_mre_sample:<12.4f} | {f_mre_sample:<12.4f} | {scale_str:<25}"     # åŸºç¡€ä¿¡æ¯å­—ç¬¦ä¸²    
                # æ¨ªå‘è¿½åŠ æ¯ä¸ªç‚¹çš„è¯¯å·®
                c_vec = c_err[0].cpu().tolist()
                f_vec = f_err[0].cpu().tolist()
                
                for k in range(config.landmarkNum):
                    val_c = c_vec[k]
                    val_f = f_vec[k]
                    # å¤„ç† NaN æ˜¾ç¤º
                    str_c = f"{val_c:.2f}" if val_c == val_c else "nan" # ä¿ç•™2ä½å°æ•°èŠ‚çœç©ºé—´
                    str_f = f"{val_f:.2f}" if val_f == val_f else "nan"
                    row_str += f" || {str_c:<10} | {str_f:<10}"
                
                logger.info(row_str)
                if (i + 1) % 10 == 0: logger.info("")

        # --- æœ€ç»ˆç»Ÿè®¡åˆ†æ ---
        # logger.info("="*50)
        logger.info("="*len(header)) # ä¿æŒå®½åº¦ä¸€è‡´
        logger.info("ğŸ“Š Final Test Results")
        logger.info("="*len(header)) # ä¿æŒå®½åº¦ä¸€è‡´
        # logger.info("="*50)
        
        if len(fine_Off) > 0:
            coarse_Off = torch.cat(coarse_Off, dim=0)
            fine_Off = torch.cat(fine_Off, dim=0)

            # ä½¿ç”¨ MyUtils.analysis_result è¿›è¡Œç»Ÿè®¡ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªå…³é”®ç‚¹çš„MREã€SDå’ŒSDR
            c_SDR, c_SD, c_MRE_list = MyUtils.analysis_result(config.landmarkNum, coarse_Off)
            f_SDR, f_SD, f_MRE_list = MyUtils.analysis_result(config.landmarkNum, fine_Off)

            # è®¡ç®—å…¨å±€å‡å€¼ (å¯¹æ‰€æœ‰å…³é”®ç‚¹çš„ MRE å†æ±‚ä¸€æ¬¡å¹³å‡) , analysis_resultè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä»£è¡¨7ä¸ªå…³é”®ç‚¹å„è‡ªçš„å¹³å‡è¯¯å·®
            c_final_mre = torch.nanmean(coarse_Off).item()  # æœ€ç§‘å­¦çš„è®¡ç®—æ–¹æ³•æ˜¯å°†æ‰€æœ‰æœ‰æ•ˆè¯¯å·®æ”¾åœ¨ä¸€èµ·å†æ±‚ä¸€æ¬¡å¹³å‡
            c_final_sd = torch.std(coarse_Off[~torch.isnan(coarse_Off)]).item() # SDæœ€åˆç†çš„è®¡ç®—ä¸æ˜¯æ ‡å‡†å·®çš„å¹³å‡å€¼ï¼Œè€Œæ˜¯è®¡ç®—å…¨å±€æ ‡å‡†å·®ï¼Œç›´æ¥å¯¹tensoræ±‚æ ‡å‡†å·®ï¼Œè€Œä¸æ˜¯å¯¹SDåˆ—è¡¨æ±‚å¹³å‡å€¼
            f_final_mre = torch.nanmean(fine_Off).item()
            f_final_sd = torch.std(fine_Off[~torch.isnan(fine_Off)]).item()
            
            logger.info(f"âœ… Global Statistics:")
            logger.info(f"   Coarse Stage -> MRE: {c_final_mre:.4f} Â± {c_final_sd:.4f} mm")
            logger.info(f"   Fine   Stage -> MRE: {f_final_mre:.4f} Â± {f_final_sd:.4f} mm")
            logger.info("-" * 71)

            # --- è¯¦ç»†çš„å…³é”®ç‚¹ç»Ÿè®¡è¡¨ (Per-Landmark Table) ---
            logger.info(f"âœ… Per-Landmark Statistics (Average over {len(fine_Off)} samples):")
            sum_header = f"{'ID':<4} | {'Name':<6} || {'Coarse MRE':<12} | {'Coarse SD':<10} || {'Fine MRE':<12} | {'Fine SD':<10}"
            logger.info(sum_header)
            logger.info("-" * len(sum_header))

            # éå†æ¯ä¸ªå…³é”®ç‚¹è¾“å‡º
            for k in range(config.landmarkNum):
                name = landmark_names[k]
                # å¤„ç† Tensor æˆ– float
                cm = c_MRE_list[k] if isinstance(c_MRE_list, list) else c_MRE_list[k].item()
                cs = c_SD[k] if isinstance(c_SD, list) else c_SD[k].item()
                fm = f_MRE_list[k] if isinstance(f_MRE_list, list) else f_MRE_list[k].item()
                fs = f_SD[k] if isinstance(f_SD, list) else f_SD[k].item()
                logger.info(f"{k:<4} | {name:<6} || {cm:<12.4f} | {cs:<10.4f} || {fm:<12.4f} | {fs:<10.4f}")
            logger.info("-" * len(sum_header))
            
            # æ‰“å°è¯¦ç»† SDR
            logger.info(f"âœ… Fine Stage SDR (Success Detection Rate):")
            # å–æ‰€æœ‰å…³é”®ç‚¹ SDR çš„å¹³å‡å€¼ä½œä¸ºå…¨å±€ SDR
            mean_sdr = torch.mean(f_SDR, dim=0) * 100
            thresholds = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] # å¯¹åº” MyUtils é‡Œçš„å®šä¹‰
            sdr_str_list = [f"{th}mm: {val:.2f}%" for th, val in zip(thresholds, mean_sdr)]
            for i in range(0, len(sdr_str_list), 4):
                logger.info("   " + " | ".join(sdr_str_list[i:i+4]))
            
        # logger.info("="*50)
        logger.info("="*len(header))

        time_elapsed = time.time() - since
        logger.info('test complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))