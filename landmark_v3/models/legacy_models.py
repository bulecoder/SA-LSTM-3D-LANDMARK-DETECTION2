import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F

# --- ğŸ”¥ ä¿®æ”¹ import è·¯å¾„ ---
# å‡è®¾åŸæ¥çš„ MyNetworkLayer.py å†…å®¹ä¹Ÿæ•´åˆè¿›æ¥äº†ï¼Œæˆ–è€…æ˜¯å•ç‹¬çš„æ–‡ä»¶
# å¦‚æœä½ æ‰“ç®—åˆå¹¶æ–‡ä»¶ï¼ŒæŠŠ MNL çš„å†…å®¹è´´åœ¨è¿™é‡Œã€‚
# å¦‚æœä¸æƒ³åˆå¹¶ï¼Œè¯·æŠŠ MyNetworkLayer.py æ”¾åˆ° models/ ç›®å½•ä¸‹å¹¶æ”¹åä¸º layers.py
# è¿™é‡Œå‡è®¾ä½ ä¿ç•™äº† MyNetworkLayer çš„ç‹¬ç«‹æ€§ (æ¨è)
from . import legacy_layers as MNL  # å‡è®¾ä½ æŠŠ MyNetworkLayer é‡å‘½åä¸º legacy_layers.py æ”¾åœ¨åŒçº§ç›®å½•
from utils import legacy_utils as MyUtils # å¼•ç”¨æ—§çš„å·¥å…·ç®±

class coarseNet(nn.Module):
    def __init__(self, config):
        super(coarseNet, self).__init__()
        self.landmarkNum = config.landmarkNum
        self.usegpu = config.use_gpu
        self.image_scale = config.image_scale
        self.u_net = MNL.U_Net3D(1, 64)
        self.dropout = nn.Dropout3d(p=0.5)  # 30%æ¦‚ç‡ä¸¢å¼ƒç‰¹å¾ï¼Œæ­£åˆ™åŒ–æŠ‘åˆ¶è¿‡æ‹Ÿåˆ
        self.conv3d = nn.Sequential(
            nn.Conv3d(64, config.landmarkNum, 1, 1, 0),
            nn.Sigmoid(),
        )

    def forward(self, x):
        # 1. éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
        global_features = self.u_net(x)  # x: (B, 1, D, H, W)
        global_features_drop = self.dropout(global_features)    # åœ¨è¿›å…¥æœ€åçš„é¢„æµ‹å±‚ä¹‹å‰ï¼Œåº”ç”¨dropout
        # 2. Conv3D+æ¿€æ´»+é˜²é™¤é›¶ï¼ˆä¼˜åŒ–epsilonï¼Œç»Ÿä¸€ä¸º1e-8ï¼‰
        x = self.conv3d(global_features_drop)  # x: (B, landmarkNum, D, H, W)
        epsilon = 1e-9
        x = x + epsilon  # æ›¿æ¢åŸä»£ç çš„+1e-9ï¼Œç»Ÿä¸€epsilon
        # 3. ä¿®å¤ç»´åº¦å±•å¹³+æ±‚å’Œï¼ˆä¿ç•™æ‰¹æ¬¡ç»´åº¦ï¼Œæ ¸å¿ƒä¿®å¤ï¼‰
        batch_size = x.shape[0]  # åŠ¨æ€è·å–æ‰¹æ¬¡å¤§å°ï¼Œé€‚é…ä»»æ„B
        flat_x = x.view(batch_size, self.landmarkNum, -1)  # (B, landmarkNum, D*H*W)
        heatmap_sum = torch.sum(flat_x, dim=2)  # ä»…å¯¹ç©ºé—´ç»´åº¦æ±‚å’Œï¼Œshape: (B, landmarkNum)
        # 5. å½’ä¸€åŒ–ï¼ˆä¿ç•™åˆ—è¡¨æ¨å¯¼å¼ï¼Œä¿®å¤å¹¿æ’­é™¤æ³•ï¼‰
        global_heatmap = [
            # x[:,i,...]ï¼šé€‚é…ä»»æ„æ‰¹æ¬¡ï¼Œæ›¿æ¢åŸx[0,i,...]
            # heatmap_sum[:,i].view(...)ï¼šæ‰©å……ç»´åº¦å®ç°å¹¿æ’­é™¤æ³•ï¼Œé€‚é…(B,D,H,W)
            x[:, i, :, :, :] / heatmap_sum[:, i].view(batch_size, 1, 1, 1)
            for i in range(self.landmarkNum)
        ]
        return global_heatmap, global_features      # è¿”å›çš„ features æ˜¯åŸå§‹çš„ global_featuresï¼Œæ²¡æœ‰åŠ dropoutï¼Œè¿™æ ·ä¼ ç»™fineNetçš„ç‰¹å¾æ˜¯å®Œæ•´çš„ï¼Œä¸ä¼šç¼ºä¿¡æ¯

class fine_LSTM(nn.Module):
    def __init__(self, config):
        super(fine_LSTM, self).__init__()

        # landmarkNum, use_gpu, iteration, cropSize

        self.landmarkNum = config.landmarkNum
        self.usegpu = config.use_gpu
        self.encoder = MNL.U_Net3D_encoder(1, 64)
        self.iteration = config.iteration
        self.crop_size = config.crop_size
        self.origin_image_size = config.origin_image_size
        self.config = config

        w, h, l = self.origin_image_size
        # (576, 768, 768)

        self.size_tensor = torch.tensor([1 / (l - 1), 1 / (h - 1), 1 / (w - 1)]).cuda(self.usegpu)
        # è¿™é‡Œçš„è¾“å…¥ç»´åº¦æ˜¯ 512 (Local) + 64 (Global) = 576
        self.decoders_offset_x = nn.Conv1d(self.landmarkNum, self.landmarkNum, 512 + 64, 1, 0, groups=self.landmarkNum)
        self.decoders_offset_y = nn.Conv1d(self.landmarkNum, self.landmarkNum, 512 + 64, 1, 0, groups=self.landmarkNum)
        self.decoders_offset_z = nn.Conv1d(self.landmarkNum, self.landmarkNum, 512 + 64, 1, 0, groups=self.landmarkNum)


        self.attention_gate_share = nn.Sequential(
            nn.Linear(512 + 64, 256),
            nn.Tanh(),
            # nn.Linear(256, 1)
            # nn.Conv1d(landmarkNum, landmarkNum, 256, 1, 0, groups=landmarkNum),
        )
        self.attention_gate_head = nn.Conv1d(self.landmarkNum, self.landmarkNum, 256, 1, 0, groups=self.landmarkNum)
        self.graph_attention = MNL.graph_attention(64, self.usegpu)
        # self.graph_attention = MNL.graph_attention(512 + 64, self.usegpu)   # å°†å…¨å±€ç‰¹å¾å’Œå±€éƒ¨ç‰¹å¾è¿›è¡Œæ‹¼æ¥åï¼Œä¸€èµ·é€å…¥GNNè¿›è¡Œäº¤äº’

    def forward(self, coarse_landmarks, labels, inputs_origin, coarse_feature, phase, size_tensor_inv):

        # cropedtems = MyUtils.getcropedInputs_related(ROIs, labels, inputs_origin, -1, 0)
        # cropedtems = torch.cat([cropedtems[i].cuda(self.usegpu) for i in range(len(cropedtems))], dim=0)
        # features = self.encoder(cropedtems).squeeze().unsqueeze(0)
        # global_feature = MyUtils.get_global_feature(ROIs, coarse_feature)
        # global_feature = self.graph_attention(ROIs, global_feature)
        # features = torch.cat((features, global_feature),dim=2)
        # x, y, z = self.decoders_offset_x(features), self.decoders_offset_y(features), self.decoders_offset_z(features)
        # predict = torch.cat([x, y, z], dim=2) * self.size_tensor.cuda(self.usegpu) + torch.from_numpy(ROIs).cuda(self.usegpu)

        h_state = 0
        predicts = []
        c_state = 0
        predict = coarse_landmarks.detach()

        for i in range(0, self.iteration):
            ROIs = 0
            # if phase == 'train':    # teacher forcingï¼Œä¸‹ä¸€æ¬¡ ROI ä½ç½®æ˜¯åŸºäºçœŸå€¼+å™ªå£°ï¼Œè€Œä¸æ˜¯ä¸Šä¸€æ¬¡çš„é¢„æµ‹
            #     if i == 0:
            #         ROIs = labels + torch.from_numpy(np.random.normal(loc=0.0, scale=32.0 / self.origin_image_size[2] / 3, size = labels.size())).cuda(self.usegpu).float()
            #     elif i == 1:
            #         ROIs = labels + torch.from_numpy(np.random.normal(loc=0.0, scale=16.0 / self.origin_image_size[2] / 3, size = labels.size())).cuda(self.usegpu).float()
            #     else:
            #         ROIs = labels + torch.from_numpy(np.random.normal(loc=0.0, scale=8.0 / self.origin_image_size[2] / 3, size = labels.size())).cuda(self.usegpu).float()
            # else:
            #     ROIs = predict
            if phase == 'train':
                # 1. è®¡ç®—å™ªå£°æ¯”ä¾‹ (ä¿æŒè®ºæ–‡çš„å¤šåˆ†è¾¨ç‡é€»è¾‘: 32 -> 16 -> 8)
                if i == 0:   scale_val = 32.0   # å¯¹åº”è®ºæ–‡é‡Œé¢çš„å¤šåˆ†è¾¨ç‡ï¼Œè¿™é‡Œæ˜¯åŠå¾„
                elif i == 1: scale_val = 16.0
                else:        scale_val = 8.0
                
                # ç”Ÿæˆå™ªå£°
                noise = torch.from_numpy(np.random.normal(
                    loc=0.0, 
                    scale=scale_val / self.origin_image_size[2] / 6,        # å™ªå£°æ¯”ä¾‹è®¾ç½®ä¸ºåŠå¾„çš„1/6
                    size=labels.size()
                )).cuda(self.usegpu).float()

                # 2. ç¡®å®šåˆ‡å›¾ä¸­å¿ƒ (ROIs)
                if i == 0:
                    # ç¬¬ 0 æ­¥ï¼šå†·å¯åŠ¨ï¼Œå¿…é¡»ç”¨ çœŸå€¼ + å™ªå£° (å¦åˆ™å¯èƒ½åˆ‡åˆ°å…¨é»‘)
                    ROIs = labels + noise
                else:
                    # ä½¿ç”¨ä¸Šä¸€æ­¥çš„é¢„æµ‹å€¼ (predict) + å™ªå£°  Student Forcingï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¼šä»ä¸Šä¸€æ­¥çš„ä½ç½®ä¿®æ­£
                    ROIs = predict.detach() + noise
            else:
                ROIs = predict  # éªŒè¯/æµ‹è¯•é˜¶æ®µï¼šå§‹ç»ˆä½¿ç”¨ä¸Šä¸€æ­¥çš„é¢„æµ‹

            ROIs = MyUtils.adjustment(ROIs, labels)
            # è¿™é‡ŒåŠ ä¸€ä¸ªæ•°å€¼è£å‰ªï¼ˆé™å¹…ï¼‰ï¼Œé¿å…ROIsè¶Šç•Œ
            ROIs = torch.clamp(ROIs, 0.0, 1.0)

            cropedtems = MyUtils.getcropedInputs_related(ROIs.detach().cpu().numpy(), labels, inputs_origin, -1, i, self.config)
            cropedtems = torch.cat([cropedtems[i].cuda(self.usegpu) for i in range(len(cropedtems))], dim=0)
            # 1. è·å– Encoder çš„åŸå§‹è¾“å‡º
            features_raw = self.encoder(cropedtems)     # å¦‚æœ crop_size=32ï¼Œå½¢çŠ¶æ˜¯ [B, 512, 1, 1, 1]ï¼›å¦‚æœ crop_size=64ï¼Œå½¢çŠ¶æ˜¯ [B, 512, 2, 2, 2]  
            # 2. å¼ºåˆ¶å‹ç¼©æˆ 1x1x1
            features_pooled = torch.nn.functional.adaptive_avg_pool3d(features_raw, (1, 1, 1))  # æ— è®º crop_size å¤šå¤§ï¼Œè¿™é‡Œè¾“å‡ºå½¢çŠ¶æ°¸è¿œæ˜¯ [B, 512, 1, 1, 1]
            # 3. è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…åç»­å…¨è¿æ¥å±‚
            features = features_pooled.view(features_pooled.size(0), -1).unsqueeze(0)   # ä¸¥è°¨å†™æ³•ï¼šå…ˆå±•å¹³ä¸º [B, 512]ï¼Œå† unsqueeze

            global_feature = MyUtils.get_global_feature(ROIs.detach().cpu().numpy(), coarse_feature, self.landmarkNum) # è·å–å…¨å±€ç‰¹å¾ï¼ˆ64ç»´åº¦ï¼‰

            # å…ˆæ‹¼æ¥å…¨å±€ç‰¹å¾+å±€éƒ¨ç‰¹å¾ï¼Œå†GNN çš„ç‰ˆæœ¬ï¼Œæµ‹è¯•åå‘ç°å˜åŒ–ä¸å¤§
            # features = torch.cat((features, global_feature), dim=2)     # æ‹¼æ¥: 512 + 64 = 576 ç»´
            # features = self.graph_attention(ROIs, features)

            # åŸå§‹ç‰ˆæœ¬ï¼šå…ˆå¯¹å…¨å±€ç‰¹å¾GNNï¼Œå†æ‹¼æ¥å±€éƒ¨ç‰¹å¾
            global_feature = self.graph_attention(ROIs, global_feature)     # graph attentionï¼ˆGNNï¼‰
            features = torch.cat((features, global_feature), dim=2)
            # features = self.graph_attention(ROIs, features)

            # h_state = features
            # c_state = ROIs
            if i == 0:
                h_state = features
                c_state = ROIs
            else:
                gate_f = self.attention_gate_head(self.attention_gate_share(h_state.squeeze()).unsqueeze(0))
                gate_a = self.attention_gate_head(self.attention_gate_share(features.squeeze()).unsqueeze(0))
                gate = torch.softmax(torch.cat([gate_f, gate_a], dim=2), dim=2)

                h_state = h_state * gate[0, :, 0].view(1, -1, 1) + features * gate[0, :, 1].view(1, -1, 1)
                c_state = c_state * gate[0, :, 0].view(1, -1, 1) + ROIs * gate[0, :, 1].view(1, -1, 1)
                # c_state = ROIs

            x, y, z = self.decoders_offset_x(h_state), self.decoders_offset_y(h_state), self.decoders_offset_z(h_state)
            # print(size_tensor_inv)
            predict = torch.cat([x, y, z], dim=2) * size_tensor_inv + c_state
            predicts.append(predict.float())

        predicts = torch.cat(predicts, dim=0)

        return predicts # è¿”å›çš„æ˜¯æ‰€æœ‰è¿­ä»£çš„ç»“æœ

# --- ğŸ”¥ ä¸ºäº†å…¼å®¹æ–°ä»£ç çš„ namingï¼Œæ·»åŠ åˆ«å ---
CoarseNet = coarseNet
FineNet = fine_LSTM