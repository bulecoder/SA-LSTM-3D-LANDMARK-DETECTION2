from __future__ import print_function, division
import torch
import time
import MyUtils
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import os

def train_model(coarse_net, fine_LSTM, dataloaders, criterion_coarse, criterion_fine, optimizer, config):
    since = time.time()
    test_epoch = 1         # epochä¸º5çš„å€æ•°çš„æ—¶å€™ï¼ŒéªŒè¯æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ
    best_mre = float('inf')     # æœ€ä½³MRE

    # --- 1. å‡†å¤‡ä¿å­˜è·¯å¾„ ---
    save_dir = os.path.join('runs', config.saveName)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # --- 2. åˆå§‹åŒ– Logger (å…³é”®ä¿®æ”¹) ---
    # æ—¥å¿—æ–‡ä»¶å°†ä¿å­˜åœ¨ runs/ä½ çš„å®éªŒå/train.log
    log_path = os.path.join(save_dir, 'train.log')
    # è°ƒç”¨æˆ‘ä»¬åœ¨ MyUtils é‡Œå†™çš„å‡½æ•°
    logger = MyUtils.get_logger(log_path) 
    logger.info(f"ğŸš€ Start Training: {config.saveName}")
    logger.info(f"ğŸ“ Logs and weights will be saved to: {save_dir}")
    logger.info("")

    # åˆå§‹åŒ– SummaryWriter
    writer = SummaryWriter(log_dir=os.path.join('runs', config.saveName))
    # tensorboardè®°å½•é…ç½®å‚æ•°
    config_str = " | Parameter | Value |\n|---|---|\n"
    for key, value in vars(config).items():
        config_str += f"| {key} | {str(value)} |\n"
    writer.add_text('Experiment_Config', config_str, 0)

    # é…ç½®å‚æ•°ä¹Ÿè®°å½•åˆ° logger
    logger.info("------ Experiment Configuration ------")
    for key, value in vars(config).items():
        logger.info(f"{key}: {value}")
    logger.info("--------------------------------------")
    logger.info("")

    # --- å‡†å¤‡å…¨å±€åæ ‡ç½‘æ ¼ ---
    gl, gh, gw = config.image_scale
    global_coordinate = torch.ones(gl, gh, gw, 3).float()
    for i in range(gl): global_coordinate[i, :, :, 0] *= i
    for i in range(gh): global_coordinate[:, i, :, 1] *= i
    for i in range(gw): global_coordinate[:, :, i, 2] *= i
    global_coordinate = global_coordinate.cuda(config.use_gpu) * torch.tensor([1 / (gl - 1), 1 / (gh - 1), 1 / (gw - 1)]).cuda(config.use_gpu)

    # --- è®­ç»ƒå¾ªç¯ ---
    for epoch in range(config.epochs):
        print() # æ¯è½®å¼€å§‹åœ¨æ§åˆ¶å°æ‰“å°ä¸€ä¸ªç©ºè¡Œéš”å¼€ï¼Œlogé‡Œé¢ä¸ç”¨ç®¡
        train_coarse_Off = []
        train_fine_Off = []
        test_coarse_Off = []
        test_fine_Off = []
        
        for phase in ['train', 'val']:
            datas = dataloaders[phase]  # ç›´æ¥è·å– DataLoader
            pbar = tqdm(total=len(datas), desc=f'{phase} Epoch {epoch}') # æ‰‹åŠ¨ç®¡ç† tqdmï¼Œä¿æŒåŸå§‹é£æ ¼

            if phase == 'train':
                coarse_net.train(True) 
                fine_LSTM.train(True)
            else:
                if epoch % test_epoch != 0: continue
                coarse_net.train(False) 
                fine_LSTM.train(False)

            lent = len(datas)
            running_loss = 0

            # éå†æ•°æ®
            for data in datas:
                inputs = data['DICOM'].cuda(config.use_gpu) # (B, C, D, H, W)
                inputs_origin_list = data['DICOM_origin']
                inputs_origin = [item.squeeze(0) for item in inputs_origin_list]
                labels = data['landmarks'].cuda(config.use_gpu) 
           
                size = data['size'][0]
                px_z, px_y, px_x = size[0].item(), size[1].item(), size[2].item()
                
                # æ„é€ åƒç´ å°ºå¯¸å¼ é‡
                size_tensor_pixel = torch.tensor([px_x, px_y, px_z]).float().cuda(config.use_gpu).unsqueeze(0)   # Label é¡ºåºæ˜¯ [X, Y, Z],ç¼©æ”¾å› å­ä¹Ÿå¿…é¡»å¯¹åº” [Width, Height, Depth]
                size_tensor_inv = 1.0 / size_tensor_pixel.float()

                # ç‰©ç†å°ºå¯¸å¼ é‡ (ç”¨äºè®¡ç®— MRE æ¯«ç±³è¯¯å·®)  é€»è¾‘: åƒç´ æ•° * Spacing = æ¯«ç±³æ•°
                sp_z, sp_y, sp_x = config.spacing # ä»é…ç½®è¯»å–
                physical_scale = torch.tensor([
                    px_x * sp_x, # Width (mm)
                    px_y * sp_y, # Height (mm)
                    px_z * sp_z  # Depth (mm)
                ]).float().cuda(config.use_gpu).unsqueeze(0)
                
                optimizer.zero_grad()

                # æ˜¾å­˜æ§åˆ¶ï¼šéªŒè¯é˜¶æ®µä¸æ„å»ºè®¡ç®—å›¾ é˜²æ­¢éªŒè¯é›†åƒæ‰æ˜¾å­˜
                with torch.set_grad_enabled(phase == 'train'):
                    # å‰å‘ä¼ æ’­ (Forward)
                    coarse_heatmap, coarse_feature = coarse_net(inputs)

                    # ç¬¬ä¸€é“å…³å¡ï¼šæ£€æŸ¥ç½‘ç»œè¾“å‡ºæ˜¯å¦æ­£å¸¸ 
                    has_nan = any(torch.isnan(h).any() for h in coarse_heatmap)     # æ£€æŸ¥ list ä¸­ä»»ä½•ä¸€ä¸ª tensor æ˜¯å¦æœ‰ NaN
                    if has_nan:     # å¦‚æœ CoarseNet è¾“å‡ºé‡Œå°±æœ‰ NaNï¼Œè¯´æ˜ç½‘ç»œå†…éƒ¨ç‚¸äº† æ­¤æ—¶å¿…é¡»è·³è¿‡ï¼Œä¸èƒ½æŠŠ NaN ä¼ ç»™ MyUtilsï¼Œå¦åˆ™ä¼šçˆ†å†…å­˜
                        logger.warning(f"âš ï¸ [Warning] NaN detected in CoarseNet output at Epoch {epoch}. Skipping this batch.")
                        optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
                        continue # ğŸ”¥ ç›´æ¥è·³è¿‡ï¼ä¸è·‘ FineNetï¼Œä¸åå‘ä¼ æ’­
                    
                    # è·å–ç²—å®šä½åæ ‡
                    coarse_landmarks = MyUtils.get_coordinates_from_coarse_heatmaps(coarse_heatmap, global_coordinate)      # è¿™é‡Œæœ‰æ²¡æœ‰.unsqueeze(0)
                    # å¼ºåˆ¶é™åˆ¶åæ ‡åœ¨ 0-1 ï¼Œé˜²æ­¢é¢„æµ‹è·‘å‡ºè¾¹ç•Œ
                    coarse_landmarks = torch.clamp(coarse_landmarks, 0.0, 1.0)
                    
                    # Fine Stage
                    fine_landmarks_all = fine_LSTM(coarse_landmarks, labels, inputs_origin, coarse_feature, phase, size_tensor_inv)

                    # è®¡ç®— Loss (Original Logic)
                    mask_loss = (labels[:, :, 0] >= 0).float().unsqueeze(2)
                    # å–æœ€åä¸€æ¬¡è¿­ä»£çš„ç»“æœæ¥è®¡ç®—æŸå¤±
                    fine_pred_last = fine_landmarks_all[-1].unsqueeze(0)
                    loss = (torch.abs(fine_pred_last - labels) * mask_loss).sum() / (mask_loss.sum() + 1e-6)

                    # Coarse Loss: ä¼ å…¥ List ç±»å‹çš„ coarse_heatmap
                    loss += criterion_coarse(coarse_heatmap, global_coordinate, labels, phase)

                    # åå‘ä¼ æ’­
                    if phase == 'train' and config.stage == 'train':
                        # ç¬¬äºŒé“å…³å¡ï¼šæ£€æŸ¥ Loss æ˜¯å¦æ­£å¸¸
                        if torch.isnan(loss):
                            logger.warning(f"âš ï¸ [Warning] Loss is NaN at Epoch {epoch}. Skipping gradient update.")
                            optimizer.zero_grad()
                            continue # ğŸ”¥ è·³è¿‡æ›´æ–°
                        loss.backward()
                        # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´ä¸‹ä¸€æ¬¡é¢„æµ‹é£å‡ºå¤©é™…)
                        torch.nn.utils.clip_grad_norm_(coarse_net.parameters(), max_norm=5.0)
                        torch.nn.utils.clip_grad_norm_(fine_LSTM.parameters(), max_norm=5.0)
                        optimizer.step()

                # -------------------------------------------------------------------
                # 4. æŒ‡æ ‡ç»Ÿè®¡
                # -------------------------------------------------------------------
                if epoch % test_epoch == 0:
                    fine_landmarks_final = fine_landmarks_all[-1].unsqueeze(0)    # åªå–æœ€åä¸€æ¬¡è¿­ä»£çš„ç»“æœç”¨äºè¯„ä¼°
                    coarse_off = MyUtils.get_coarse_errors(coarse_landmarks, labels, physical_scale) # è®¡ç®—è¯¯å·®ï¼Œç›´æ¥ä¼ å…¥ç‰©ç†å°ºå¯¸ï¼ˆmmï¼‰
                    fine_off = MyUtils.get_fine_errors(fine_landmarks_final, labels, physical_scale)

                    # å¤„ç†ç¼ºå¤±å€¼ï¼šå¡«å…… NaN
                    mask_valid = (labels[:, :, 0] >= 0) # [1, N]
                    coarse_off[~mask_valid] = float('nan')
                    fine_off[~mask_valid] = float('nan')
                    
                    if phase == "train":
                        train_fine_Off.append(fine_off.detach().cpu())
                        train_coarse_Off.append(coarse_off.detach().cpu())
                    else:
                        test_fine_Off.append(fine_off.detach().cpu())
                        test_coarse_Off.append(coarse_off.detach().cpu())
                
                running_loss += loss.item()
                pbar.update(1)

            # End of Epoch
            epoch_loss = running_loss / lent
            pbar.close()
            
            if epoch % 1 == 0:
                logger.info('{} epoch: {} Loss: {:.4f}'.format(phase, epoch, epoch_loss))
            
            if phase == 'train':
                writer.add_scalar('Loss/Train', epoch_loss, epoch)
            elif phase == 'val':
                writer.add_scalar('Loss/Val', epoch_loss, epoch)

        # -------------------------------------------------------------------
        # 5. TensorBoard è®°å½•ä¸ç»“æœä¿å­˜
        # -------------------------------------------------------------------
        if epoch % test_epoch == 0:
            current_test_mre = float('inf')
            # --- å†…éƒ¨å‡½æ•°ï¼šè®¡ç®—å¹¶è®°å½•æŒ‡æ ‡ (é¿å…ä»£ç é‡å¤) ---
            def process_stats(tensor_list, prefix):
                if len(tensor_list) == 0: return float('inf'), float('inf'), None
                # 1. æ‹¼æ¥ [Total_N, 7] (åŒ…å« NaN)
                all_tensor = torch.cat(tensor_list, dim=0)
                # 2. MyUtils è®¡ç®—ç»†èŠ‚ (SDR, æ¯åˆ—å‡å€¼)
                SDR, _, _ = MyUtils.analysis_result(config.landmarkNum, all_tensor)
                # 3. è®¡ç®—å…¨å±€æŒ‡æ ‡ (Micro-Average, å¿½ç•¥ NaN)
                global_mre = torch.nanmean(all_tensor).item()
                # è®¡ç®—å…¨å±€ SD (å…¼å®¹æ—§ç‰ˆ PyTorch çš„æ‰‹åŠ¨ nanstd)
                global_sd = torch.std(all_tensor[~torch.isnan(all_tensor)]).item()
                # 4. è®°å½• TensorBoard
                writer.add_scalar(f'{prefix}_MRE', global_mre, epoch)
                writer.add_scalar(f'{prefix}_SD', global_sd, epoch)
                # è®°å½• SDR (å–æ‰€æœ‰å…³é”®ç‚¹å¹³å‡)
                sdr_mean = torch.mean(SDR, dim=0) * 100
                writer.add_scalar(f'{prefix}_SDR/2.0mm', sdr_mean[1], epoch) # å‡è®¾é˜ˆå€¼ç´¢å¼•1å¯¹åº”2.0mm
                writer.add_scalar(f'{prefix}_SDR/4.0mm', sdr_mean[3], epoch)
                writer.add_scalar(f'{prefix}_SDR/6.0mm', sdr_mean[5], epoch)
                writer.add_scalar(f'{prefix}_SDR/8.0mm', sdr_mean[7], epoch)
                return global_mre, global_sd, sdr_mean
            # --- å†…éƒ¨å‡½æ•°ï¼šè¾“å‡ºæŒ‡æ ‡ç»“æœ (é¿å…ä»£ç é‡å¤) ---
            def print_detailed_results(title, c_mre, c_sd, f_mre, f_sd, sdr_vec):
                logger.info(f"   [{title} Results]")
                logger.info(f"   Fine   -> MRE: {f_mre:.4f} mm | SD: {f_sd:.4f} mm")
                logger.info(f"   Coarse -> MRE: {c_mre:.4f} mm | SD: {c_sd:.4f} mm")
                logger.info(f"   SDR (Thresholds):")
                logger.info(f"     2.0mm:{sdr_vec[1]:.2f}%")
                logger.info(f"     4.0mm:{sdr_vec[3]:.2f}%")
                logger.info(f"     6.0mm:{sdr_vec[5]:.2f}%")
                logger.info(f"     8.0mm:{sdr_vec[7]:.2f}%")
                logger.info(f"   Full SDR Vector: {sdr_vec.tolist()}")
                logger.info("")
                
            # å¤„ç† Train
            c_mre_train, c_sd_train, _ = process_stats(train_coarse_Off, 'Train/Coarse')
            f_mre_train, f_sd_train, sdr_train = process_stats(train_fine_Off, 'Train/Fine')

            # å¤„ç† Val/Test ç»“æœ
            c_mre_test, c_sd_test, _ = process_stats(test_coarse_Off, 'Test/Coarse')
            f_mre_test, f_sd_test, sdr_test = process_stats(test_fine_Off, 'Test/Fine')
            
            # è®°å½•å¯¹æ¯”æ›²çº¿
            writer.add_scalars('Comparison/MRE_Train', {'Coarse': c_mre_train, 'Fine': f_mre_train}, epoch)
            writer.add_scalars('Comparison/MRE_Test', {'Coarse': c_mre_test, 'Fine': f_mre_test}, epoch)

            current_test_mre = f_mre_test # ä»¥ Fine MRE ä¸ºå‡†

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_test_mre < best_mre:
                logger.info(f"ğŸ”¥ New Best! MRE: {best_mre:.4f} -> {current_test_mre:.4f}")
                best_mre = current_test_mre
                torch.save(coarse_net.state_dict(), os.path.join(save_dir, 'best_coarse.pth'))
                torch.save(fine_LSTM.state_dict(), os.path.join(save_dir, 'best_fine_LSTM.pth'))
            
            if (epoch + 1) % 10 == 0:       # æ¯10ä¸ªepochæ‰“å°è¾“å‡ºä¸€ä¸‹è¯„ä»·æŒ‡æ ‡
                logger.info("")
                print_detailed_results("TRAIN", c_mre_train, c_sd_train, f_mre_train, f_sd_train, sdr_train)
                print_detailed_results("TEST ", c_mre_test, c_sd_test, f_mre_test, f_sd_test, sdr_test)
            
            # ä¿å­˜æœ€æ–°æ¨¡å‹ (é˜²æ­¢ä¸­æ–­)
            torch.save(coarse_net.state_dict(), os.path.join(save_dir, 'latest_coarse.pth'))
            torch.save(fine_LSTM.state_dict(), os.path.join(save_dir, 'latest_fine_LSTM.pth'))

        logger.info("")     # æ‰“å°ç©ºè¡Œï¼Œä¸ºäº†ç»ˆç«¯æ˜¾ç¤ºç¾è§‚ï¼Œæ—¥å¿—é‡Œé¢ä¼šæœ‰ä¸ªç©ºè¡Œ
        torch.cuda.empty_cache()
    
    time_elapsed = time.time() - since
    logger.info('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    writer.close()


def test_model(coarse_net, fine_LSTM, dataloader, config):      # æš‚æ—¶æ²¡æœ‰åŠ å…¥criterion1, criterion2, optimizer, è¿™ä¸‰ä¸ªå‚æ•°
    since = time.time()

    coarse_net.eval()
    fine_LSTM.eval()
    
    # 1. å‡†å¤‡å…¨å±€åæ ‡ç½‘æ ¼ (ä¸ TrainNet ä¿æŒä¸€è‡´)
    gl, gh, gw = config.image_scale
    global_coordinate = torch.ones(gl, gh, gw, 3).float()
    for i in range(gl): global_coordinate[i, :, :, 0] *= i
    for i in range(gh): global_coordinate[:, i, :, 1] *= i
    for i in range(gw): global_coordinate[:, :, i, 2] *= i
    global_coordinate = global_coordinate.cuda(config.use_gpu) * torch.tensor([1 / (gl - 1), 1 / (gh - 1), 1 / (gw - 1)]).cuda(config.use_gpu)

    # å®¹å™¨
    coarse_Off = []
    fine_Off = []

    # --- 1. å‡†å¤‡ä¿å­˜è·¯å¾„ ---
    save_dir = os.path.join('runs', config.testName)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # --- 2. åˆå§‹åŒ– Logger ---   æ—¥å¿—æ–‡ä»¶å°†ä¿å­˜åœ¨ runs/ä½ çš„å®éªŒå/test.log
    log_path = os.path.join(save_dir, 'test.log')
    # è°ƒç”¨æˆ‘ä»¬åœ¨ MyUtils é‡Œå†™çš„å‡½æ•°
    logger = MyUtils.get_logger(log_path) 
    logger.info(f"ğŸš€ Start Testing: {config.testName}")
    logger.info(f"ğŸ“ Logs will be saved to: {save_dir}")
    logger.info(f"{'Sample ID':<25} | {'Coarse MRE':<12} | {'Fine MRE':<12} | {'Scale (mm)':<25}")
    logger.info("-" * 80)

    with torch.no_grad():
        for i, data in enumerate(dataloader):
            inputs = data['DICOM'].cuda(config.use_gpu)
            labels = data['landmarks'].cuda(config.use_gpu)
            inputs_origin_list = data['DICOM_origin']
            inputs_origin = [item.squeeze(0) for item in inputs_origin_list]
            
            # --- æ„å»ºå°ºå¯¸å¼ é‡ ---
            # 1. è·å– Batch ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å°ºå¯¸ (å‡è®¾ Batch=1)
            size_data = data['size'][0] 
            px_z, px_y, px_x = size_data[0].item(), size_data[1].item(), size_data[2].item()
            
            # 2. åƒç´ å°ºå¯¸å¼ é‡ (ç”¨äº FineNet è¾“å…¥/å½’ä¸€åŒ–) - é¡ºåº [W, H, D] å¯¹åº” [x, y, z]
            size_tensor_pixel = torch.tensor([px_x, px_y, px_z]).float().cuda(config.use_gpu).unsqueeze(0)
            size_tensor_inv = 1.0 / size_tensor_pixel

            # 3. ç‰©ç†å°ºå¯¸å¼ é‡ (ç”¨äºè®¡ç®— MRE æ¯«ç±³è¯¯å·®)
            sp_z, sp_y, sp_x = config.spacing
            physical_scale = torch.tensor([
                px_x * sp_x, # Width (mm)
                px_y * sp_y, # Height (mm)
                px_z * sp_z  # Depth (mm)
            ]).float().cuda(config.use_gpu).unsqueeze(0)

            # --- æ¨ç† ---
            # 1. Coarse Stage
            coarse_heatmap, coarse_feature = coarse_net(inputs)
            
            # 2. Get Coarse Coordinates
            # coarse_landmarks = MyUtils.get_coordinates_from_coarse_heatmaps(coarse_heatmap, global_coordinate).unsqueeze(0)
            coarse_landmarks = MyUtils.get_coordinates_from_coarse_heatmaps(coarse_heatmap, global_coordinate)
            coarse_landmarks = torch.clamp(coarse_landmarks, 0.0, 1.0)
            
            # 3. Fine Stage
            fine_landmarks = fine_LSTM(coarse_landmarks, labels, inputs_origin, coarse_feature, 'test', size_tensor_inv)    # è¿™é‡Œå¾—åˆ°çš„ä¸‰æ¬¡è¿­ä»£çš„ä¸‰ä¸ªç»“æœ
            fine_landmarks = fine_landmarks[-1].unsqueeze(0)    # åªå–æœ€åä¸€æ¬¡è¿­ä»£çš„ç»“æœ

            # --- è®¡ç®—è¯¯å·® (mm) ---     è¿™é‡Œçš„errå½¢çŠ¶æ˜¯ (B, N)ï¼ŒåŒ…å«äº†æ— æ•ˆç‚¹çš„å·¨å¤§è¯¯å·®
            c_err = MyUtils.get_coarse_errors(coarse_landmarks, labels, physical_scale)
            f_err = MyUtils.get_fine_errors(fine_landmarks, labels, physical_scale)

            # ä½¿ç”¨ Mask è¿‡æ»¤æ— æ•ˆç‚¹ï¼Œä¿ç•™æœ‰æ•ˆç‚¹ï¼Œæ— æ•ˆä½ç½®è®¾ç½®ä¸º nan
            mask = (labels[:, :, 0] >= 0) # å½¢çŠ¶ (B, N)
            c_err[~mask] = float('nan')
            f_err[~mask] = float('nan')
            
            # è®°å½•æ•°æ®
            coarse_Off.append(c_err.cpu())
            fine_Off.append(f_err.cpu())
            
            # æ‰“å°å•ä¸ªæ ·æœ¬ä¿¡æ¯ (å¯é€‰)
            sample_name = data['imageName'][0] if 'imageName' in data else "Unknown"
            c_mre_sample = torch.nanmean(c_err).item()      # torch.nanmeanå¯ä»¥ç›´æ¥å¤„ç† tensorä¸­çš„ nan
            f_mre_sample = torch.nanmean(f_err).item()
            scale_str = f"[{physical_scale[0,0]:.1f}, {physical_scale[0,1]:.1f}, {physical_scale[0,2]:.1f}]"
            logger.info(f"{sample_name[:25]:<25} | {c_mre_sample:<12.4f} | {f_mre_sample:<12.4f} | {scale_str:<25}")
            if (i + 1) % 10 == 0: logger.info("")

    # --- æœ€ç»ˆç»Ÿè®¡åˆ†æ ---
    logger.info("="*50)
    logger.info("ğŸ“Š Final Test Results")
    logger.info("="*50)
    
    if len(fine_Off) > 0:
        coarse_Off = torch.cat(coarse_Off, dim=0)
        fine_Off = torch.cat(fine_Off, dim=0)

        # ä½¿ç”¨ MyUtils.analysis_result è¿›è¡Œç»Ÿè®¡ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªå…³é”®ç‚¹çš„MREã€SDå’ŒSDR
        c_SDR, c_SD, c_MRE_list = MyUtils.analysis_result(config.landmarkNum, coarse_Off)
        f_SDR, f_SD, f_MRE_list = MyUtils.analysis_result(config.landmarkNum, fine_Off)

        # è®¡ç®—å…¨å±€å‡å€¼ (å¯¹æ‰€æœ‰å…³é”®ç‚¹çš„ MRE å†æ±‚ä¸€æ¬¡å¹³å‡) , analysis_resultè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä»£è¡¨7ä¸ªå…³é”®ç‚¹å„è‡ªçš„å¹³å‡è¯¯å·®
        c_final_mre = torch.nanmean(coarse_Off).item()  # æœ€ç§‘å­¦çš„è®¡ç®—æ–¹æ³•æ˜¯å°†æ‰€æœ‰æœ‰æ•ˆè¯¯å·®æ”¾åœ¨ä¸€èµ·å†æ±‚ä¸€æ¬¡å¹³å‡
        c_final_sd = torch.std(coarse_Off[~torch.isnan(coarse_Off)]).item() # SDæœ€åˆç†çš„è®¡ç®—ä¸æ˜¯æ ‡å‡†å·®çš„å¹³å‡å€¼ï¼Œè€Œæ˜¯è®¡ç®—å…¨å±€æ ‡å‡†å·®ï¼Œç›´æ¥å¯¹tensoræ±‚æ ‡å‡†å·®ï¼Œè€Œä¸æ˜¯å¯¹SDåˆ—è¡¨æ±‚å¹³å‡å€¼
        f_final_mre = torch.nanmean(fine_Off).item()
        f_final_sd = torch.std(fine_Off[~torch.isnan(fine_Off)]).item()
        
        logger.info(f"âœ… Coarse Stage:")
        logger.info(f"   MRE: {c_final_mre:.4f} mm")
        logger.info(f"   SD: {c_final_sd:.4f} mm")
        
        logger.info(f"âœ… Fine Stage:")
        logger.info(f"   MRE: {f_final_mre:.4f} mm")
        logger.info(f"   SD: {f_final_sd:.4f} mm")
        
        # æ‰“å°è¯¦ç»† SDR
        logger.info(f"âœ… Fine Stage SDR (Success Detection Rate):")
        # å–æ‰€æœ‰å…³é”®ç‚¹ SDR çš„å¹³å‡å€¼ä½œä¸ºå…¨å±€ SDR
        mean_sdr = torch.mean(f_SDR, dim=0) * 100
        
        thresholds = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] # å¯¹åº” MyUtils é‡Œçš„å®šä¹‰
        for i, th in enumerate(thresholds):
            logger.info(f"   {th}mm: {mean_sdr[i]:.2f}%")
        
    logger.info("="*50)

    time_elapsed = time.time() - since
    logger.info('test complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
